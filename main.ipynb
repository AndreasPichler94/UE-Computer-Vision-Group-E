{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "last_valid_sequence = 0\n",
    "\n",
    "def split_data(train_percentage, part_folder):\n",
    "    print(\"Splitting data...\")\n",
    "\n",
    "    if not os.path.exists(training_data_path):\n",
    "        os.makedirs(training_data_path)\n",
    "\n",
    "    if not os.path.exists(validation_data_path):\n",
    "        os.makedirs(validation_data_path)\n",
    "\n",
    "    prefixes = set()\n",
    "    for filename in os.listdir(os.path.join(raw_directory_path, part_folder)):\n",
    "        prefix = re.match(r'^(\\d+)_', filename)\n",
    "        if prefix:\n",
    "            prefixes.add(prefix.group(1))\n",
    "\n",
    "    for prefix in prefixes:\n",
    "\n",
    "        file_count = len([f for f in os.listdir(os.path.join(raw_directory_path, part_folder))\n",
    "                          if f.startswith(f'{prefix}_') and os.path.isfile(os.path.join(raw_directory_path, part_folder, f))])\n",
    "\n",
    "        total_sequences = round(file_count / 13) #TODO: Wenn hier eine ungerade Zahl herauskommt, wurden nicht alle Files richtig sortiert.\n",
    "        train_size = round((file_count / 13) * train_percentage)\n",
    "\n",
    "        # Copy files according to the split into training and validation data directories.\n",
    "        for current_index in range(1, total_sequences):\n",
    "            expected_files = get_expected_files(current_index, part_folder, prefix)\n",
    "\n",
    "            if current_index < train_size:\n",
    "                destination_path = training_data_path\n",
    "            else:\n",
    "                destination_path = validation_data_path\n",
    "\n",
    "            for expected_file in expected_files:\n",
    "                source_path = os.path.join(raw_directory_path, expected_file)\n",
    "                destination_file = os.path.join(destination_path, os.path.basename(expected_file))\n",
    "\n",
    "                # Check if the file exists before attempting to move it\n",
    "                if os.path.exists(source_path):\n",
    "                    shutil.move(source_path, destination_file)\n",
    "                #else:\n",
    "                    #print(f\"File not found: {source_path}\")\n",
    "\n",
    "    if not last_valid_sequence == 0:\n",
    "        print(f\"Data split into {train_percentage * 100}% training ({train_size} Sequences) and {100 - train_percentage * 100}% validation ({total_sequences - train_size} Sequences).\")\n",
    "\n",
    "    print(f\"{os.path.join(raw_directory_path, part_folder)} Splitting complete!\")\n",
    "\n",
    "def get_expected_files(current_index, part_folder, prefix):\n",
    "    expected_files = set()\n",
    "\n",
    "    for i in range(11):\n",
    "        expected_files.add(f'{prefix}_{current_index}_pose_{i}_thermal.png')\n",
    "\n",
    "    expected_files.add(f'{prefix}_{current_index}_Parameters.txt')\n",
    "    expected_files.add(f'{prefix}_{current_index}_GT_pose_0_thermal.png')\n",
    "\n",
    "    return [os.path.join(part_folder, file) for file in expected_files]\n",
    "\n",
    "def rename_files(current_index, part_folder, prefix):\n",
    "    expected_files = get_expected_files(current_index, part_folder, prefix)\n",
    "    for expected_file in expected_files:\n",
    "        old_path = os.path.join(raw_directory, expected_file)\n",
    "        new_index = last_valid_sequence + 1\n",
    "        new_file = expected_file.replace(f'{prefix}_{current_index}', f'{prefix}_{new_index}')\n",
    "        new_path = os.path.join(raw_directory, new_file)\n",
    "        os.rename(old_path, new_path)\n",
    "    #print(f\"Renamed sequences with index {current_index} to {last_valid_sequence + 1}\")\n",
    "    return new_index\n",
    "\n",
    "def cleanup_data(part_folder):\n",
    "    print(f\"{raw_directory} - Cleaning started...\")\n",
    "\n",
    "    global last_valid_sequence\n",
    "    invalid_sequences_found = False\n",
    "\n",
    "    prefixes = set()\n",
    "    for filename in os.listdir(os.path.join(raw_directory_path, part_folder)):\n",
    "        prefix = re.match(r'^(\\d+)_', filename)\n",
    "        if prefix:\n",
    "            prefixes.add(prefix.group(1))\n",
    "\n",
    "    for prefix in prefixes:\n",
    "        last_valid_sequence = 0\n",
    "\n",
    "        file_count = len([f for f in os.listdir(os.path.join(raw_directory_path, part_folder))\n",
    "                          if f.startswith(f'{prefix}_') and os.path.isfile(os.path.join(raw_directory_path, part_folder, f))])\n",
    "\n",
    "        # TODO better calculation for sequences where more than 50% of the Files are missing.\n",
    "        approximated_sequences = math.ceil(file_count / 13)\n",
    "\n",
    "        for current_index in range(approximated_sequences):\n",
    "            expected_files = get_expected_files(current_index, part_folder, prefix)\n",
    "            missing_file = None\n",
    "            found_files = [file_name for file_name in expected_files if\n",
    "                           os.path.exists(os.path.join(raw_directory_path, part_folder, file_name))]\n",
    "\n",
    "            if len(found_files) > 0:\n",
    "                for file_name in expected_files:\n",
    "                    full_path = os.path.join(raw_directory_path, part_folder, file_name)\n",
    "                    if not os.path.exists(full_path):\n",
    "                        missing_file = file_name\n",
    "                        break\n",
    "\n",
    "                if missing_file is not None:\n",
    "                    invalid_sequences_found = True\n",
    "                    for delete_file in expected_files:\n",
    "                        delete_path = os.path.join(raw_directory_path, part_folder, delete_file)\n",
    "                        if os.path.exists(delete_path):\n",
    "                            os.remove(delete_path)\n",
    "                    print(f\"Sequence {current_index} (Prefix {prefix}) deleted due to missing file: {missing_file}\")\n",
    "                else:\n",
    "                    if current_index != 0 and (current_index - last_valid_sequence) > 1:\n",
    "                        last_valid_sequence = rename_files(current_index, part_folder, prefix)\n",
    "                    else:\n",
    "                        last_valid_sequence = current_index\n",
    "\n",
    "    if not invalid_sequences_found:\n",
    "        print(\"No invalid sequences found.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #TODO Unzip all the zipped batches into ./raw/data\n",
    "    raw_directory_path = os.path.join(os.path.abspath(os.path.join(os.path.abspath(__file__), '..', '..', '..')), 'data', 'raw')\n",
    "    training_data_path = os.path.join(os.path.abspath(os.path.join(os.path.abspath(__file__), '..', '..', '..')), 'data', 'train')\n",
    "    validation_data_path = os.path.join(os.path.abspath(os.path.join(os.path.abspath(__file__), '..', '..', '..')), 'data', 'test')\n",
    "    train_percentage = 0.8\n",
    "\n",
    "    # Process Folders\n",
    "    for part_folder in [\"Part1\", \"Part2\"]:\n",
    "        raw_directory = os.path.join(raw_directory_path, part_folder)\n",
    "\n",
    "        # Data processing\n",
    "        cleanup_data(raw_directory)\n",
    "        split_data(train_percentage, raw_directory) #TODO: Improving.\n",
    "        print(\"##########################################\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# AOS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_4616\\3412774377.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0m__name__\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m\"__main__\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m     \u001B[0maktuelles_verzeichnis\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mabspath\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdirname\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m__file__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0maktuelles_verzeichnis\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import math\n",
    "import glm\n",
    "import pyaos\n",
    "import re\n",
    "import glob\n",
    "\n",
    "def eul2rotm(theta):\n",
    "    s_1 = math.sin(theta[0])\n",
    "    c_1 = math.cos(theta[0])\n",
    "    s_2 = math.sin(theta[1])\n",
    "    c_2 = math.cos(theta[1])\n",
    "    s_3 = math.sin(theta[2])\n",
    "    c_3 = math.cos(theta[2])\n",
    "    rotm = np.identity(3)\n",
    "    rotm[0, 0] = c_1 * c_2\n",
    "    rotm[0, 1] = c_1 * s_2 * s_3 - s_1 * c_3\n",
    "    rotm[0, 2] = c_1 * s_2 * c_3 + s_1 * s_3\n",
    "\n",
    "    rotm[1, 0] = s_1 * c_2\n",
    "    rotm[1, 1] = s_1 * s_2 * s_3 + c_1 * c_3\n",
    "    rotm[1, 2] = s_1 * s_2 * c_3 - c_1 * s_3\n",
    "\n",
    "    rotm[2, 0] = -s_2\n",
    "    rotm[2, 1] = c_2 * s_3\n",
    "    rotm[2, 2] = c_2 * c_3\n",
    "\n",
    "    return rotm\n",
    "\n",
    "\n",
    "def createviewmateuler(eulerang, camLocation):\n",
    "    rotationmat = eul2rotm(eulerang)\n",
    "    translVec = np.reshape((-camLocation @ rotationmat), (3, 1))\n",
    "    conjoinedmat = (np.append(np.transpose(rotationmat), translVec, axis=1))\n",
    "    return conjoinedmat\n",
    "\n",
    "\n",
    "def divide_by_alpha(rimg2):\n",
    "    a = np.stack((rimg2[:, :, 3], rimg2[:, :, 3], rimg2[:, :, 3]), axis=-1)\n",
    "    return rimg2[:, :, :3] / a\n",
    "\n",
    "\n",
    "def pose_to_virtualcamera(vpose):\n",
    "    vp = glm.mat4(*np.array(vpose).transpose().flatten())\n",
    "    # vp = vpose.copy()\n",
    "    ivp = glm.inverse(glm.transpose(vp))\n",
    "    # ivp = glm.inverse(vpose)\n",
    "    Posvec = glm.vec3(ivp[3])\n",
    "    Upvec = glm.vec3(ivp[1])\n",
    "    FrontVec = glm.vec3(ivp[2])\n",
    "    lookAt = glm.lookAt(Posvec, Posvec + FrontVec, Upvec)\n",
    "    cameraviewarr = np.asarray(lookAt)\n",
    "    # print(cameraviewarr)\n",
    "    return cameraviewarr\n",
    "\n",
    "def numericalSort(value):\n",
    "    numbers = re.compile(r'(\\d+)')\n",
    "    parts = numbers.split(value)\n",
    "    parts[1::2] = map(int, parts[1::2])\n",
    "    return parts\n",
    "\n",
    "\n",
    "def generate_poses(aos, integral_Path, render_fov):\n",
    "    ########################## Below we generate the poses for rendering #####################################\n",
    "    # This is based on how renderer is implemented.\n",
    "\n",
    "    Numberofimages = 11  # Or just the number of images\n",
    "    Focal_plane = 0  # Focal plane is set to the ground, so it is zero.\n",
    "\n",
    "    # ref_loc is the reference location or the poses of the images. The poses are the same for the dataset and therefore only the images have to be replaced.\n",
    "    ref_loc = [[5, 4, 3, 2, 1, 0, -1, -2, -3, -4, -5], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]  # These are the x and y positions of the images. It is of the form [[x_positions],[y_positions]]\n",
    "\n",
    "    altitude_list = [35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35]  # [Z values which is the height]\n",
    "\n",
    "    center_index = 5  # this is important, this will be the pose index at which the integration should happen. For example if you have 5 images, lets say you want to integrate all 5 images to the second image position. Then your center_index is 1 as index starts from zero.\n",
    "\n",
    "    site_poses = []\n",
    "    for i in range(Numberofimages):\n",
    "        EastCentered = (ref_loc[0][i] - 0.0)    # Get MeanEast and Set MeanEast\n",
    "        NorthCentered = (0.0 - ref_loc[1][i])   # Get MeanNorth and Set MeanNorth\n",
    "        M = createviewmateuler(np.array([0.0, 0.0, 0.0]), np.array([ref_loc[0][i], ref_loc[1][i], - altitude_list[i]]))\n",
    "        print('m', M)\n",
    "        ViewMatrix = np.vstack((M, np.array([0.0, 0.0, 0.0, 1.0], dtype=np.float32)))\n",
    "        print(ViewMatrix)\n",
    "        camerapose = np.asarray(ViewMatrix.transpose(), dtype=np.float32)\n",
    "        print(camerapose)\n",
    "        site_poses.append(camerapose)  # site_poses is a list now containing all the poses of all the images in a certain format that is accepted by the renderer.\n",
    "\n",
    "    # Read the generated images from the simulator and store in a list\n",
    "\n",
    "    numbers = re.compile(r'(\\d+)')\n",
    "\n",
    "    imagelist = []\n",
    "\n",
    "    for img in sorted(glob.glob(os.path.join(training_data_path, '*.png')), key=numericalSort):\n",
    "        n = cv2.imread(img)\n",
    "        imagelist.append(n)\n",
    "\n",
    "    aos.clearViews()  # Every time you call the renderer you should use this line to clear the previous views\n",
    "    for i in range(len(imagelist)):\n",
    "        aos.addView(imagelist[i], site_poses[i], \"DEM BlobTrack\")  # Here we are adding images to the renderer one by one.\n",
    "    aos.setDEMTransform([0, 0, Focal_plane])\n",
    "\n",
    "    proj_RGBimg = aos.render(pose_to_virtualcamera(site_poses[center_index]), render_fov)\n",
    "    tmp_RGB = divide_by_alpha(proj_RGBimg)\n",
    "    cv2.imwrite(os.path.join(integral_Path, 'integral.png'),\n",
    "                tmp_RGB)  # Final result. Check the integral result in the integrals folder.\n",
    "\n",
    "\n",
    "def aos_renderer():\n",
    "    w, h, fovDegrees = 512, 512, 50  # resolution and field of view. This should not be changed.\n",
    "    render_fov = 50\n",
    "\n",
    "    if 'window' not in locals() or window == None:\n",
    "        window = pyaos.PyGlfwWindow(w, h, 'AOS')\n",
    "\n",
    "    aos = pyaos.PyAOS(w, h, fovDegrees)\n",
    "\n",
    "    set_folder = os.path.abspath(os.path.join(os.path.abspath(os.path.dirname(__file__)), os.pardir, 'python'))\n",
    "    aos.loadDEM(os.path.join(set_folder, 'zero_plane.obj'))\n",
    "    return aos, render_fov\n",
    "\n",
    "\n",
    "def check_directories():\n",
    "    try:\n",
    "        script_path = os.path.abspath(__file__)\n",
    "    except NameError:\n",
    "        script_path = None\n",
    "\n",
    "    if script_path is not None:\n",
    "        Download_Location = os.path.abspath(os.path.join(os.path.dirname(os.path.dirname(script_path)), 'results'))\n",
    "    else:\n",
    "        Download_Location = os.path.abspath(os.path.join(os.getcwd(), '..', 'results'))\n",
    "\n",
    "    print(\"Download Location:\", Download_Location)\n",
    "\n",
    "    Integral_Path = os.path.join(Download_Location, 'integrals')\n",
    "\n",
    "    if not os.path.exists(Integral_Path):\n",
    "        os.mkdir(Integral_Path)\n",
    "    else:\n",
    "        print(f\"The directory '{Integral_Path}' already exists.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    training_data_path = os.path.join(os.path.abspath(os.path.join(os.path.abspath(__file__), '..', '..', '..')), 'data','train')\n",
    "\n",
    "    integral_path = check_directories()\n",
    "    aos, render_fov = aos_renderer()\n",
    "    generate_poses(aos, integral_path, render_fov)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./models')\n",
    "sys.path.append('./utils')\n",
    "\n",
    "import torch\n",
    "\n",
    "from aos_deeplab import AosDeepLab\n",
    "from train_deeplab import train_deeplab, check_gpu_availability\n",
    "\n",
    "model = AosDeepLab()\n",
    "print(f'GPU available: {check_gpu_availability()}')\n",
    "\n",
    "trained_model = train_deeplab(model, num_epochs=10)\n",
    "\n",
    "# torch.save(trained_model.state_dict(), \"aosdeeplab_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# checkpoints"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def get_checkpoint(model, optimizer, checkpoint_dir = \"./checkpoints\"):\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint.pth')\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        current_index = checkpoint['current_index']\n",
    "        current_batch = checkpoint['current_batch']\n",
    "        print(f\"Checkpoint loaded. Current index: {current_index}, Current batch: {current_batch}\")\n",
    "        return current_index, current_batch\n",
    "    else:\n",
    "        print(\"No checkpoint found. Training from scratch.\")\n",
    "        return 0, 0\n",
    "    \n",
    "def save_checkpoint(model, optimizer, current_index, current_batch, checkpoint_dir='./checkpoints'):\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint.pth')\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'current_index': current_index,\n",
    "        'current_batch': current_batch,\n",
    "    }, checkpoint_path)\n",
    "\n",
    "    data = {\"current_index\": current_index, \"current_batch\": current_batch}\n",
    "    with open(os.path.join(checkpoint_dir, 'checkpoint_info.pickle'), 'wb') as handle:\n",
    "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
